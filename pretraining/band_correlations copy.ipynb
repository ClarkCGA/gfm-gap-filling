{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b33749e-1e66-4d79-a7c1-ae7d1e146125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mae.models_mae\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from mae_training import CombinedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b32ec0e-86c0-45eb-9dd3-ea44263dd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set local rank and define mean and std tensors for normalization purposes\n",
    "local_rank = 1\n",
    "mean = torch.tensor([495.7316,  814.1386,  924.5740, 2962.5623, 2640.8833, 1740.3031])[None,:,None,None,None].to(local_rank)\n",
    "std = torch.tensor([286.9569, 359.3304, 576.3471, 892.2656, 945.9432, 916.1625])[None,:,None,None,None].to(local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9868345e-fb88-49ae-b368-588d326cf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that prepares the model based on the checkpoint and the correct architecture\n",
    "def prepare_model(checkpoint, arch='mae_vit_base_patch16'):\n",
    "        # build model\n",
    "        model = getattr(mae.models_mae, arch)()\n",
    "        # load model\n",
    "        checkpoint_file = torch.load(checkpoint, map_location=f'cuda:{local_rank}')\n",
    "        msg = model.load_state_dict(checkpoint_file, strict=False)\n",
    "        print(msg)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52941d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with the job ID of the experiment you want to visualize\n",
    "job_id = \"6231-fair-bs16-2023-08-21_16-49-34\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106cb1e7-9e15-46c6-8505-6a3e7230ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the YAML file\n",
    "yaml_file_path = Path(f\"/workspace/data/lchu/hls/jobs/{job_id}.yaml\")\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "training_length = yaml_data[\"training_length\"]\n",
    "save_dir = Path(yaml_data[\"visualization_dir\"])\n",
    "checkpoint = Path(yaml_data[\"checkpoint_dir\"]) / \"model_best.pt\"\n",
    "experiment_name = f\"{training_length} Chips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c373cf-393a-485b-8884-072652088eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define which checkpoint, name of experiment, and save directory we will use for zero shot\n",
    "# experiment_name = \"Zero-Shot\"\n",
    "# checkpoint = Path(\"/workspace/gfm-gap-filling/pretraining/epoch-832-loss-0.0473.pt\")\n",
    "# save_dir = Path(\"/workspace/data/lchu/hls/vis/zero_shot_visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d967fc9-b5b4-405a-a624-2bf2e4d57439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# prepare model\n",
    "model = prepare_model(checkpoint, 'mae_vit_base_patch16')\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a206a9ae-8fa2-44f4-bddf-ff41d2e76e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation dataset\n",
    "val_dataset = CombinedDataset(\"/workspace/gfm-gap-filling/pretraining/train_single_band/train_single_band\", split=\"validate\", num_frames=3, img_size=224, bands=6, cloud_range=[0.01,1.0],\n",
    "                              # random_cropping=random_cropping, remove_cloud=True, \n",
    "                               normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b419d1-1829-4409-897a-5c13161914ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Validation set len = 1621\n",
      "--> Validation set masks = 1600\n"
     ]
    }
   ],
   "source": [
    "# ensure the length and number of masks are correct\n",
    "print(f\"--> Validation set len = {len(val_dataset)}\")\n",
    "print(f\"--> Validation set masks = {val_dataset.n_cloudpaths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea90fee-8ed7-40b5-bc63-e368d9b548b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send model to device at local rank\n",
    "torch.cuda.set_device(local_rank)\n",
    "model = model.to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57398f5-e1e8-418d-ba9f-aa06b86077a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first item: torch.Size([1, 2, 6, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# get the low coverage image from the val dataset and ensure the shape is correct\n",
    "first_batch = torch.from_numpy(val_dataset[640][np.newaxis, ...]).to(local_rank)\n",
    "print(\"Shape of the first item:\", first_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d63e3e-fc32-4840-bc36-651fe3a098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on low coverage image\n",
    "label_mask_batch = first_batch[:,1,:,:,:,:].to(local_rank)\n",
    "batch = first_batch[:,0,:,:,:,:].to(local_rank)\n",
    "loss, pred, mask = model(batch, label_mask_batch, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e57f13d-b1fb-4f8d-97ea-bb37118939c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we un-normalize and re-normalize to reflectance values with scaling factor normalization\n",
    "# the scaling factor for hls data is 0.0001\n",
    "# we use torch.ceil() to avoid floating point errors resulting in negative values\n",
    "input = torch.ceil((batch.detach() * std) + mean) * 0.0001\n",
    "input_mask = label_mask_batch.detach()\n",
    "predicted = torch.ceil(model.unpatchify(pred).detach() * std + mean) * 0.0001\n",
    "\n",
    "# use masks to create input, predicted, and non-cloud tensors where values we don't need are set as 0\n",
    "input_masked = input * input_mask\n",
    "predicted_masked = predicted * input_mask\n",
    "non_cloud = input * (1-input_mask)\n",
    "\n",
    "# send tensors to numpy\n",
    "input_masked = input_masked.cpu().numpy()\n",
    "predicted_masked = predicted_masked.cpu().numpy()\n",
    "non_cloud = non_cloud.cpu().numpy()\n",
    "\n",
    "# set values that are masked out to nan so the are not counted in visualizations\n",
    "input_masked[input_masked == 0] = np.nan\n",
    "predicted_masked[predicted_masked == 0] = np.nan\n",
    "non_cloud[non_cloud == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a137a1-469c-403e-938a-a55ceb2af184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130/1478839957.py:36: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_1130/1478839957.py:46: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_1130/1478839957.py:56: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "### creating a pairgrid for the low coverage image\n",
    "\n",
    "# putting the data into a dataframe where each column represents an ordered list of band values\n",
    "# the tensors are in format (Batch, Channel, Time Step, Height, Width)\n",
    "# therefore, we select the first batch of 1, each channel sequentially, the second time step, and all pixels within H and W\n",
    "non_cloud_data = pd.DataFrame({\n",
    "    'B2': non_cloud[0,0,1,:,:].flatten(),\n",
    "    'B3': non_cloud[0,1,1,:,:].flatten(),\n",
    "    'B4': non_cloud[0,2,1,:,:].flatten(),\n",
    "    'B5': non_cloud[0,3,1,:,:].flatten(),\n",
    "    'B7': non_cloud[0,4,1,:,:].flatten(),\n",
    "    'B8': non_cloud[0,5,1,:,:].flatten()\n",
    "})\n",
    "gen_data = pd.DataFrame({\n",
    "    'B2': predicted_masked[0,0,1,:,:].flatten(),\n",
    "    'B3': predicted_masked[0,1,1,:,:].flatten(),\n",
    "    'B4': predicted_masked[0,2,1,:,:].flatten(),\n",
    "    'B5': predicted_masked[0,3,1,:,:].flatten(),\n",
    "    'B7': predicted_masked[0,4,1,:,:].flatten(),\n",
    "    'B8': predicted_masked[0,5,1,:,:].flatten()\n",
    "})\n",
    "true_data = pd.DataFrame({\n",
    "    'B2': input_masked[0,0,1,:,:].flatten(),\n",
    "    'B3': input_masked[0,1,1,:,:].flatten(),\n",
    "    'B4': input_masked[0,2,1,:,:].flatten(),\n",
    "    'B5': input_masked[0,3,1,:,:].flatten(),\n",
    "    'B7': input_masked[0,4,1,:,:].flatten(),\n",
    "    'B8': input_masked[0,5,1,:,:].flatten()\n",
    "})\n",
    "\n",
    "# define 20 regular bin edges from 0 to 1 in increments of 0.05\n",
    "bin_edges = [round(i * 0.05, 2) for i in range(20)]\n",
    "\n",
    "# first pairgrid: true data, low coverage\n",
    "true_data_pairgrid = sns.PairGrid(true_data, diag_sharey=False)\n",
    "true_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "true_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Ground Truth Pixels\\nViT, {experiment_name}, Low Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_ground_truth_low_coverage.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "# second pairgrid: generated data, low coverage\n",
    "gen_data_pairgrid = sns.PairGrid(gen_data, diag_sharey=False)\n",
    "gen_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "gen_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Generated Pixels\\nViT, {experiment_name}, Low Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_generated_low_coverage.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "# third pairgrid: non-masked data, low coverage\n",
    "non_cloud_pairgrid = sns.PairGrid(non_cloud_data, diag_sharey=False)\n",
    "non_cloud_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "non_cloud_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Non-Cloud Pixels\\nViT, {experiment_name}, Low Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_non_cloud_low_coverage.png', format='png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77d986e9-e7ed-4b09-9acd-25ecd3f6595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1621\n"
     ]
    }
   ],
   "source": [
    "# Setting up the dataset sampler the same as during training, with a batch size of 1\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_dataset)\n",
    "test_kwargs = {\"batch_size\": 1, \"sampler\": val_sampler}\n",
    "common_kwargs = {\n",
    "        \"pin_memory\": False,\n",
    "        \"drop_last\": True\n",
    "    }\n",
    "test_kwargs.update(common_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92ebcbcc-16bc-49a1-aba0-b2fbf6613571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:10<00:00, 19.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# adding pixel values to tensors iteratively\n",
    "\n",
    "# initialize empty tensors to which we will concatenate pixel values for each band\n",
    "# as we have 6 bands, we initialize empty tensors of size (6, 0)\n",
    "# the tensors will be (Channel, B*H*W)\n",
    "true_pixels = torch.empty((6, 0)).to(local_rank)\n",
    "gen_pixels = torch.empty((6, 0)).to(local_rank)\n",
    "non_cloud_pixels = torch.empty((6, 0)).to(local_rank)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for idx, batch in tqdm(enumerate(test_loader), initial = 1, total=200):\n",
    "    # get mask batches from dataset\n",
    "    label_mask_batch = batch[:,1,:,:,:,:].to(local_rank)\n",
    "\n",
    "    # get input image batches from dataset\n",
    "    batch = batch[:,0,:,:,:,:].to(local_rank)\n",
    "    \n",
    "    # run model\n",
    "    loss, pred, mask = model(batch, label_mask_batch, 0.75)\n",
    "\n",
    "    # once again, we un-normalize and re-normalize to reflectance values with scaling factor normalization\n",
    "    input = torch.ceil((batch.detach() * std) + mean) * 0.0001\n",
    "    input_mask = label_mask_batch.detach()\n",
    "    predicted = torch.ceil(model.unpatchify(pred).detach() * std + mean) * 0.0001\n",
    "\n",
    "    # use masks to create input, predicted, and non-cloud tensors\n",
    "    input_masked = input * input_mask\n",
    "    predicted_masked = predicted * input_mask\n",
    "    non_cloud = input * (1-input_mask)\n",
    "\n",
    "    # use view() to create tensors of shape (Channels, B*H*W) which represents (Channel, Pixel values for entire image)\n",
    "    non_cloud_pixels_data = non_cloud[0,:,1,:,:].view(6,-1)\n",
    "    gen_pixels_data = predicted_masked[0,:,1,:,:].view(6,-1)\n",
    "    true_pixels_data = input_masked[0,:,1,:,:].view(6,-1)\n",
    "\n",
    "    # concatenate the pixel values of this batch with the running tensors of pixel values\n",
    "    true_pixels = torch.cat((true_pixels, true_pixels_data), dim=1)\n",
    "    gen_pixels = torch.cat((gen_pixels, gen_pixels_data), dim=1)\n",
    "    non_cloud_pixels = torch.cat((non_cloud_pixels, non_cloud_pixels_data), dim=1) \n",
    "    \n",
    "    # at the last iteration, send tensors to numpy and set 0 values to np.nan\n",
    "    if idx + 1 == 200:\n",
    "        true_pixels = true_pixels.cpu().numpy()\n",
    "        gen_pixels = gen_pixels.cpu().numpy()\n",
    "        non_cloud_pixels = non_cloud_pixels.cpu().numpy()\n",
    "        true_pixels[true_pixels == 0] = np.nan\n",
    "        gen_pixels[gen_pixels == 0] = np.nan\n",
    "        non_cloud_pixels[non_cloud_pixels == 0] = np.nan\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c123022-b1fd-41b3-bbab-38c56380853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting these values in a dataframe\n",
    "\n",
    "non_cloud_data = pd.DataFrame({\n",
    "    'B2': non_cloud_pixels[0],\n",
    "    'B3': non_cloud_pixels[1],\n",
    "    'B4': non_cloud_pixels[2],\n",
    "    'B5': non_cloud_pixels[3],\n",
    "    'B7': non_cloud_pixels[4],\n",
    "    'B8': non_cloud_pixels[5]\n",
    "})\n",
    "gen_data = pd.DataFrame({\n",
    "    'B2': gen_pixels[0],\n",
    "    'B3': gen_pixels[1],\n",
    "    'B4': gen_pixels[2],\n",
    "    'B5': gen_pixels[3],\n",
    "    'B7': gen_pixels[4],\n",
    "    'B8': gen_pixels[5]\n",
    "})\n",
    "true_data = pd.DataFrame({\n",
    "    'B2': true_pixels[0],\n",
    "    'B3': true_pixels[1],\n",
    "    'B4': true_pixels[2],\n",
    "    'B5': true_pixels[3],\n",
    "    'B7': true_pixels[4],\n",
    "    'B8': true_pixels[5]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a9aa642-5fc8-44c9-83d0-b7e9b27e1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130/3208497403.py:9: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_1130/3208497403.py:19: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_1130/3208497403.py:29: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "bin_edges = [round(i * 0.05, 2) for i in range(20)]\n",
    "\n",
    "true_data_pairgrid = sns.PairGrid(true_data, diag_sharey=False)\n",
    "true_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "true_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Ground Truth Pixels\\nViT, {experiment_name}, 200 Test Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_ground_truth_all.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "gen_data_pairgrid = sns.PairGrid(gen_data, diag_sharey=False)\n",
    "gen_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "gen_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Generated Pixels\\nViT, {experiment_name}, 200 Test Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_generated_all.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "non_cloud_pairgrid = sns.PairGrid(non_cloud_data, diag_sharey=False)\n",
    "non_cloud_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "non_cloud_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Non-Cloud Pixels\\nViT, {experiment_name}, 200 Test Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_non_cloud_all.png', format='png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
