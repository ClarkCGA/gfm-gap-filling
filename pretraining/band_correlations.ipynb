{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b33749e-1e66-4d79-a7c1-ae7d1e146125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mae.models_mae\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from mae_training import CombinedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b32ec0e-86c0-45eb-9dd3-ea44263dd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set local rank and define mean and std tensors for normalization purposes\n",
    "local_rank = 0\n",
    "mean = torch.tensor([495.7316,  814.1386,  924.5740, 2962.5623, 2640.8833, 1740.3031])[None,:,None,None,None].to(local_rank)\n",
    "std = torch.tensor([286.9569, 359.3304, 576.3471, 892.2656, 945.9432, 916.1625])[None,:,None,None,None].to(local_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9868345e-fb88-49ae-b368-588d326cf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that prepares the model based on the checkpoint and the correct architecture\n",
    "def prepare_model(checkpoint, arch='mae_vit_base_patch16'):\n",
    "        # build model\n",
    "        model = getattr(mae.models_mae, arch)()\n",
    "        # load model\n",
    "        checkpoint_file = torch.load(checkpoint, map_location=f'cuda:{local_rank}')\n",
    "        msg = model.load_state_dict(checkpoint_file, strict=False)\n",
    "        print(msg)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52941d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with the job ID of the experiment you want to visualize\n",
    "job_id = \"6231-fair-bs16-2024-01-25_22-04-34\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "106cb1e7-9e15-46c6-8505-6a3e7230ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the YAML file\n",
    "yaml_file_path = Path(f\"/workspace/data/lchu/hls/jobs/{job_id}.yaml\")\n",
    "with open(yaml_file_path, \"r\") as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "training_length = yaml_data[\"training_length\"]\n",
    "save_dir = Path(\"/workspace/data/lchu/hls/vis/full_6231\")\n",
    "checkpoint = Path(yaml_data[\"checkpoint_dir\"]) / \"model_best_ssim.pt\"\n",
    "experiment_name = f\"{training_length} Chips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23c373cf-393a-485b-8884-072652088eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define which checkpoint, name of experiment, and save directory we will use for zero shot\n",
    "# experiment_name = \"Zero-Shot\"\n",
    "# checkpoint = Path(\"/workspace/gfm-gap-filling/pretraining/epoch-832-loss-0.0473.pt\")\n",
    "# save_dir = Path(\"/workspace/data/lchu/hls/vis/zero_shot_visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d967fc9-b5b4-405a-a624-2bf2e4d57439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# prepare model\n",
    "model = prepare_model(checkpoint, 'mae_vit_base_patch16')\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a206a9ae-8fa2-44f4-bddf-ff41d2e76e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation dataset\n",
    "val_dataset = CombinedDataset(\"/workspace/gfm-gap-filling/pretraining/training_data\", split=\"validate\", num_frames=3, img_size=224, bands=6, cloud_range=[0.01,1.0],\n",
    "                              # random_cropping=random_cropping, remove_cloud=True, \n",
    "                               normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39b419d1-1829-4409-897a-5c13161914ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Validation set len = 1621\n",
      "--> Validation set masks = 1600\n"
     ]
    }
   ],
   "source": [
    "# ensure the length and number of masks are correct\n",
    "print(f\"--> Validation set len = {len(val_dataset)}\")\n",
    "print(f\"--> Validation set masks = {val_dataset.n_cloudpaths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ea90fee-8ed7-40b5-bc63-e368d9b548b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send model to device at local rank\n",
    "torch.cuda.set_device(local_rank)\n",
    "model = model.to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f6fa9d4-e994-4dc9-8e04-2921fd90df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first item: torch.Size([1, 2, 6, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# get the low coverage image from the val dataset and ensure the shape is correct\n",
    "first_batch = torch.from_numpy(val_dataset[80][np.newaxis, ...]).to(local_rank)\n",
    "print(\"Shape of the first item:\", first_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53d63e3e-fc32-4840-bc36-651fe3a098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on low coverage image\n",
    "label_mask_batch = first_batch[:,1,:,:,:,:].to(local_rank)\n",
    "batch = first_batch[:,0,:,:,:,:].to(local_rank)\n",
    "loss, pred, mask = model(batch, label_mask_batch, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e57f13d-b1fb-4f8d-97ea-bb37118939c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we un-normalize and re-normalize to reflectance values with scaling factor normalization\n",
    "# the scaling factor for hls data is 0.0001\n",
    "# we use torch.ceil() to avoid floating point errors resulting in negative values\n",
    "input = torch.ceil((batch.detach() * std) + mean) * 0.0001\n",
    "input_mask = label_mask_batch.detach()\n",
    "predicted = torch.ceil(model.unpatchify(pred).detach() * std + mean) * 0.0001\n",
    "\n",
    "# use masks to create input, predicted, and non-cloud tensors where values we don't need are set as 0\n",
    "input_masked = input * input_mask\n",
    "predicted_masked = predicted * input_mask\n",
    "non_cloud = input * (1-input_mask)\n",
    "\n",
    "# send tensors to numpy\n",
    "input_masked = input_masked.cpu().numpy()\n",
    "predicted_masked = predicted_masked.cpu().numpy()\n",
    "non_cloud = non_cloud.cpu().numpy()\n",
    "\n",
    "# set values that are masked out to nan so the are not counted in visualizations\n",
    "input_masked[input_masked == 0] = np.nan\n",
    "predicted_masked[predicted_masked == 0] = np.nan\n",
    "non_cloud[non_cloud == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0a137a1-469c-403e-938a-a55ceb2af184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fcc3f2b6140>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "/tmp/ipykernel_69/379124041.py:41: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_69/379124041.py:52: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_69/379124041.py:63: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "### creating a pairgrid for the low coverage image\n",
    "\n",
    "# putting the data into a dataframe where each column represents an ordered list of band values\n",
    "# the tensors are in format (Batch, Channel, Time Step, Height, Width)\n",
    "# therefore, we select the first batch of 1, each channel sequentially, the second time step, and all pixels within H and W\n",
    "non_cloud_data = pd.DataFrame({\n",
    "    'B2': non_cloud[0,0,:,:,:].flatten(),\n",
    "    'B3': non_cloud[0,1,:,:,:].flatten(),\n",
    "    'B4': non_cloud[0,2,:,:,:].flatten(),\n",
    "    'B5': non_cloud[0,3,:,:,:].flatten(),\n",
    "    'B7': non_cloud[0,4,:,:,:].flatten(),\n",
    "    'B8': non_cloud[0,5,:,:,:].flatten()\n",
    "})\n",
    "gen_data = pd.DataFrame({\n",
    "    'B2': predicted_masked[0,0,:,:,:].flatten(),\n",
    "    'B3': predicted_masked[0,1,:,:,:].flatten(),\n",
    "    'B4': predicted_masked[0,2,:,:,:].flatten(),\n",
    "    'B5': predicted_masked[0,3,:,:,:].flatten(),\n",
    "    'B7': predicted_masked[0,4,:,:,:].flatten(),\n",
    "    'B8': predicted_masked[0,5,:,:,:].flatten()\n",
    "})\n",
    "true_data = pd.DataFrame({\n",
    "    'B2': input_masked[0,0,:,:,:].flatten(),\n",
    "    'B3': input_masked[0,1,:,:,:].flatten(),\n",
    "    'B4': input_masked[0,2,:,:,:].flatten(),\n",
    "    'B5': input_masked[0,3,:,:,:].flatten(),\n",
    "    'B7': input_masked[0,4,:,:,:].flatten(),\n",
    "    'B8': input_masked[0,5,:,:,:].flatten()\n",
    "})\n",
    "\n",
    "# define 40 regular bin edges from 0 to 1 in increments of 0.025\n",
    "bin_edges = [round(i * 0.025, 3) for i in range(40)]\n",
    "\n",
    "# first pairgrid: true data, low coverage\n",
    "true_data_pairgrid = sns.PairGrid(true_data, diag_sharey=False)\n",
    "true_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "true_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Ground Truth Pixels\\nViT, {experiment_name}, Low Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_ground_truth_low_coverage.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "# second pairgrid: generated data, low coverage\n",
    "gen_data_pairgrid = sns.PairGrid(gen_data, diag_sharey=False)\n",
    "gen_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "gen_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Generated Pixels\\nViT, {experiment_name}, Low Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_generated_low_coverage.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "# third pairgrid: non-masked data, low coverage\n",
    "non_cloud_pairgrid = sns.PairGrid(non_cloud_data, diag_sharey=False)\n",
    "non_cloud_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "non_cloud_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Non-Cloud Pixels\\nViT, {experiment_name}, Low Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_non_cloud_low_coverage.png', format='png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e685018b-6456-430a-b0a7-246cd73de4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the second item: torch.Size([1, 2, 6, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# get the high coverage image from the val dataset and ensure the shape is correct\n",
    "second_batch = torch.from_numpy(val_dataset[1280][np.newaxis, ...]).to(local_rank)\n",
    "print(\"Shape of the second item:\", second_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8eed17a-051b-4145-b58f-00015cde4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on high coverage image\n",
    "label_mask_batch = second_batch[:,1,:,:,:,:].to(local_rank)\n",
    "batch = second_batch[:,0,:,:,:,:].to(local_rank)\n",
    "loss, pred, mask = model(batch, label_mask_batch, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f555a6-f848-4ea2-bae2-ee7eeae4ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we un-normalize and re-normalize to reflectance values with scaling factor normalization\n",
    "input = torch.ceil((batch.detach() * std) + mean) * 0.0001\n",
    "input_mask = label_mask_batch.detach()\n",
    "predicted = torch.ceil(model.unpatchify(pred).detach() * std + mean) * 0.0001\n",
    "\n",
    "# use masks to create input, predicted, and non-cloud tensors where values we don't need are set as 0\n",
    "input_masked = input * input_mask\n",
    "predicted_masked = predicted * input_mask\n",
    "non_cloud = input * (1-input_mask)\n",
    "\n",
    "# send tensors to numpy\n",
    "input_masked = input_masked.cpu().numpy()\n",
    "predicted_masked = predicted_masked.cpu().numpy()\n",
    "non_cloud = non_cloud.cpu().numpy()\n",
    "\n",
    "# set values that are masked out to nan so the are not counted in visualizations\n",
    "input_masked[input_masked == 0] = np.nan\n",
    "predicted_masked[predicted_masked == 0] = np.nan\n",
    "non_cloud[non_cloud == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9072842c-7879-4c19-9a81-332bda5ed7d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69/2807340340.py:41: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_69/2807340340.py:52: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_69/2807340340.py:63: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "### creating a pairgrid for the low coverage image\n",
    "\n",
    "# putting the data into a dataframe where each column represents an ordered list of band values\n",
    "# the tensors are in format (Batch, Channel, Time Step, Height, Width)\n",
    "# therefore, we select the first batch of 1, each channel sequentially, the second time step, and all pixels within H and W\n",
    "non_cloud_data = pd.DataFrame({\n",
    "    'B2': non_cloud[0,0,:,:,:].flatten(),\n",
    "    'B3': non_cloud[0,1,:,:,:].flatten(),\n",
    "    'B4': non_cloud[0,2,:,:,:].flatten(),\n",
    "    'B5': non_cloud[0,3,:,:,:].flatten(),\n",
    "    'B7': non_cloud[0,4,:,:,:].flatten(),\n",
    "    'B8': non_cloud[0,5,:,:,:].flatten()\n",
    "})\n",
    "gen_data = pd.DataFrame({\n",
    "    'B2': predicted_masked[0,0,:,:,:].flatten(),\n",
    "    'B3': predicted_masked[0,1,:,:,:].flatten(),\n",
    "    'B4': predicted_masked[0,2,:,:,:].flatten(),\n",
    "    'B5': predicted_masked[0,3,:,:,:].flatten(),\n",
    "    'B7': predicted_masked[0,4,:,:,:].flatten(),\n",
    "    'B8': predicted_masked[0,5,:,:,:].flatten()\n",
    "})\n",
    "true_data = pd.DataFrame({\n",
    "    'B2': input_masked[0,0,:,:,:].flatten(),\n",
    "    'B3': input_masked[0,1,:,:,:].flatten(),\n",
    "    'B4': input_masked[0,2,:,:,:].flatten(),\n",
    "    'B5': input_masked[0,3,:,:,:].flatten(),\n",
    "    'B7': input_masked[0,4,:,:,:].flatten(),\n",
    "    'B8': input_masked[0,5,:,:,:].flatten()\n",
    "})\n",
    "\n",
    "# define 40 regular bin edges from 0 to 1 in increments of 0.025\n",
    "bin_edges = [round(i * 0.025, 3) for i in range(40)]\n",
    "\n",
    "# first pairgrid: true data, high coverage\n",
    "true_data_pairgrid = sns.PairGrid(true_data, diag_sharey=False)\n",
    "true_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "true_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Ground Truth Pixels\\nViT, {experiment_name}, Full Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_ground_truth_full_coverage.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "# second pairgrid: generated data, high coverage\n",
    "gen_data_pairgrid = sns.PairGrid(gen_data, diag_sharey=False)\n",
    "gen_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "gen_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Generated Pixels\\nViT, {experiment_name}, Full Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_generated_full_coverage.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "# third pairgrid: non-masked data, low coverage\n",
    "non_cloud_pairgrid = sns.PairGrid(non_cloud_data, diag_sharey=False)\n",
    "non_cloud_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "non_cloud_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Non-Cloud Pixels\\nViT, {experiment_name}, Full Coverage', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_non_cloud_full_coverage.png', format='png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77d986e9-e7ed-4b09-9acd-25ecd3f6595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1621\n"
     ]
    }
   ],
   "source": [
    "# setting up the dataset sampler the same as during training, with a batch size of 1\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_dataset)\n",
    "test_kwargs = {\"batch_size\": 1, \"sampler\": val_sampler}\n",
    "common_kwargs = {\n",
    "        \"pin_memory\": False,\n",
    "        \"drop_last\": True\n",
    "    }\n",
    "test_kwargs.update(common_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92ebcbcc-16bc-49a1-aba0-b2fbf6613571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▌                                                                             | 5/64 [00:00<00:03, 17.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▊                                                                        | 9/64 [00:00<00:03, 16.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                  | 13/64 [00:00<00:03, 16.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████                                                             | 17/64 [00:00<00:02, 15.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▏                                                       | 21/64 [00:01<00:02, 15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▍                                                  | 25/64 [00:01<00:02, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████▌                                             | 29/64 [00:01<00:02, 15.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████▊                                        | 33/64 [00:01<00:01, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████▉                                   | 37/64 [00:02<00:01, 15.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████▏                             | 41/64 [00:02<00:01, 15.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████▎                        | 45/64 [00:02<00:01, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████████▌                   | 49/64 [00:03<00:00, 15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n",
      "torch.Size([1, 6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████████▌                   | 49/64 [00:03<00:00, 15.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m non_cloud_pixels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(local_rank)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(test_loader), initial \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# get mask batches from dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     label_mask_batch \u001b[38;5;241m=\u001b[39m batch[:,\u001b[38;5;241m1\u001b[39m,:,:,:,:]\u001b[38;5;241m.\u001b[39mto(local_rank)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# get input image batches from dataset\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/workspace/gfm-gap-filling/pretraining/mae_training.py:172\u001b[0m, in \u001b[0;36mCombinedDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# read in merged tif as ground truth\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m groundtruth \u001b[38;5;241m=\u001b[39m \u001b[43mread_tif_as_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtif_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# need to normalize here\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[1;32m    175\u001b[0m     groundtruth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(groundtruth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9999\u001b[39m, \u001b[38;5;241m0.0001\u001b[39m,\n\u001b[1;32m    176\u001b[0m                             (groundtruth \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd)  \u001b[38;5;66;03m# don't normalize on nodata\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/gfm-gap-filling/pretraining/mae_training.py:169\u001b[0m, in \u001b[0;36mCombinedDataset.__getitem__.<locals>.read_tif_as_np_array\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_tif_as_np_array\u001b[39m(path):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m rasterio\u001b[38;5;241m.\u001b[39mopen(path) \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[0;32m--> 169\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# adding pixel values to tensors iteratively\n",
    "\n",
    "# initialize empty tensors to which we will concatenate pixel values for each band\n",
    "# as we have 6 bands, we initialize empty tensors of size (6, 0)\n",
    "# the tensors will be (Channel, B*H*W)\n",
    "true_pixels = torch.empty((6, 0)).to(local_rank)\n",
    "gen_pixels = torch.empty((6, 0)).to(local_rank)\n",
    "non_cloud_pixels = torch.empty((6, 0)).to(local_rank)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for idx, batch in tqdm(enumerate(test_loader), initial = 1, total=64):\n",
    "    # get mask batches from dataset\n",
    "    label_mask_batch = batch[:,1,:,:,:,:].to(local_rank)\n",
    "\n",
    "    # get input image batches from dataset\n",
    "    batch = batch[:,0,:,:,:,:].to(local_rank)\n",
    "    \n",
    "    # run model\n",
    "    loss, pred, mask = model(batch, label_mask_batch, 0.75)\n",
    "\n",
    "    # once again, we un-normalize and re-normalize to reflectance values with scaling factor normalization    \n",
    "    input = torch.ceil((batch.detach() * std) + mean) * 0.0001\n",
    "    input_mask = label_mask_batch.detach()\n",
    "    predicted = torch.ceil(model.unpatchify(pred).detach() * std + mean) * 0.0001\n",
    "    \n",
    "    # use masks to create input, predicted, and non-cloud tensors\n",
    "    input_masked = input * input_mask\n",
    "    predicted_masked = predicted * input_mask\n",
    "    non_cloud = input * (1-input_mask)\n",
    "    print(non_cloud.shape)\n",
    "    # use view() to create tensors of shape (Channels, B*H*W) which represents (Channel, Pixel values for entire image)\n",
    "    non_cloud_pixels_data = non_cloud[0,:,:,:,:].view(6,-1)\n",
    "    gen_pixels_data = predicted_masked[0,:,:,:,:].view(6,-1)\n",
    "    true_pixels_data = input_masked[0,:,:,:,:].view(6,-1)\n",
    "\n",
    "    # concatenate the pixel values of this batch with the running tensors of pixel values\n",
    "    true_pixels = torch.cat((true_pixels, true_pixels_data), dim=1)\n",
    "    gen_pixels = torch.cat((gen_pixels, gen_pixels_data), dim=1)\n",
    "    non_cloud_pixels = torch.cat((non_cloud_pixels, non_cloud_pixels_data), dim=1) \n",
    "    \n",
    "    # at the last iteration, send tensors to numpy and set 0 values to np.nan\n",
    "    if idx + 1 == 64:\n",
    "        true_pixels = true_pixels.cpu().numpy()\n",
    "        gen_pixels = gen_pixels.cpu().numpy()\n",
    "        non_cloud_pixels = non_cloud_pixels.cpu().numpy()\n",
    "        true_pixels[true_pixels == 0] = np.nan\n",
    "        gen_pixels[gen_pixels == 0] = np.nan\n",
    "        non_cloud_pixels[non_cloud_pixels == 0] = np.nan\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c123022-b1fd-41b3-bbab-38c56380853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting these values in a dataframe\n",
    "\n",
    "non_cloud_data = pd.DataFrame({\n",
    "    'B2': non_cloud_pixels[0],\n",
    "    'B3': non_cloud_pixels[1],\n",
    "    'B4': non_cloud_pixels[2],\n",
    "    'B5': non_cloud_pixels[3],\n",
    "    'B7': non_cloud_pixels[4],\n",
    "    'B8': non_cloud_pixels[5]\n",
    "})\n",
    "gen_data = pd.DataFrame({\n",
    "    'B2': gen_pixels[0],\n",
    "    'B3': gen_pixels[1],\n",
    "    'B4': gen_pixels[2],\n",
    "    'B5': gen_pixels[3],\n",
    "    'B7': gen_pixels[4],\n",
    "    'B8': gen_pixels[5]\n",
    "})\n",
    "true_data = pd.DataFrame({\n",
    "    'B2': true_pixels[0],\n",
    "    'B3': true_pixels[1],\n",
    "    'B4': true_pixels[2],\n",
    "    'B5': true_pixels[3],\n",
    "    'B7': true_pixels[4],\n",
    "    'B8': true_pixels[5]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a9aa642-5fc8-44c9-83d0-b7e9b27e1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_528/2276832209.py:11: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_528/2276832209.py:22: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_528/2276832209.py:33: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "# define 40 regular bin edges from 0 to 1 in increments of 0.025\n",
    "bin_edges = [round(i * 0.025, 3) for i in range(40)]\n",
    "\n",
    "# first pairgrid: true data, all pixels in 192 images\n",
    "true_data_pairgrid = sns.PairGrid(true_data, diag_sharey=False)\n",
    "true_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='red')\n",
    "true_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "true_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Ground Truth Pixels\\nViT, {experiment_name}, 64 Test Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_ground_truth_all.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "# second pairgrid: generated data, all pixels in 192 images\n",
    "gen_data_pairgrid = sns.PairGrid(gen_data, diag_sharey=False)\n",
    "gen_data_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='blue')\n",
    "gen_data_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "gen_data_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Generated Pixels\\nViT, {experiment_name}, 64 Test Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_generated_all.png', format='png')\n",
    "plt.close()\n",
    "\n",
    "# third pairgrid: non cloud data, all pixels in 192 images\n",
    "non_cloud_pairgrid = sns.PairGrid(non_cloud_data, diag_sharey=False)\n",
    "non_cloud_pairgrid.map_lower(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.map_diag(sns.histplot, bins=bin_edges, color='green')\n",
    "non_cloud_pairgrid.set(xlim=(0, 1), ylim=(0, 1))\n",
    "non_cloud_pairgrid.fig.set_size_inches(10, 10)\n",
    "plt.suptitle(f'Relationship Between Band Reflectance Values of Non-Cloud Pixels\\nViT, {experiment_name}, 64 Test Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'band_correlations_non_cloud_all.png', format='png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a4621-756f-4d9b-801d-041814cfd52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
